name: Uncertainties
title: 1.4: Treatment of Uncertainties
lead:  Science always involves uncertainties. The complexity of the climate system and the large range of processes involved bring particular challenges.

# Treatment of Uncertainties

## Uncertainty in Environmental Science

Science always involves uncertainties. These arise at each step of the scientific method: in the development of models or hypotheses, in measurements and in analyses and interpretation of scientific assumptions. Climate science is not different in this regard from other areas of science. The complexity of the climate system and the large range of processes involved bring particular challenges because, for example, gaps in direct measurements of the past can be filled only by reconstructions using proxy data.

Because the Earth’s climate system is characterized by multiple spatial and temporal scales, uncertainties do not usually reduce at a single, predictable rate: for example, new observations may reduce the uncertainties surrounding short-timescale processes quite rapidly, while longer timescale processes may require very long observational baselines before much progress can be made. Characterization of the interaction between processes, as quantified by models, can be improved by model development, or can shed light on new areas in which uncertainty is greater than previously thought. The fact that there is only a single realization of the climate, rather than a range of different climates from which to draw, can matter significantly for certain lines of enquiry, most notably for the detection and attribution of causes of climate change and for the evaluation of projections of future states.

## Characterizing Uncertainty

‘Uncertainty’ is a complex and multifaceted property, sometimes originating in a lack of information, and at other times from quite fundamental disagreements about what is known or even knowable ({'Moss_and_Schneider_2000 Moss and Schneider, 2000}). Furthermore, scientists often disagree about the best or most appropriate way to characterize these uncertainties: some can be quantified easily while others cannot. Moreover, appropriate characterization is dependent on the intended use of the information and the particular needs of that user community.
Scientific uncertainty can be partitioned in various ways, in which the details of the partitioning usually depend on the context. For instance, the process and classifications used for evaluating observational uncertainty in climate science is not the same as that employed to evaluate projections of future change. Uncertainty in measured quantities can arise from a range of sources, such as statistical variation, variability, inherent randomness, inhomogeneity, approximation, subjective judgement, and linguistic imprecision ({'Morgan_et_al_1990 Morgan et al., 1990}), or from calibration methodologies, instrumental bias or instrumental limitations ({'JCGM_2008 JCGM, 2008}).

In the modelling studies that underpin projections of future climate change, it is common to partition uncertainty into four main categories: scenario uncertainty, due to uncertainty of future emissions of GHGs and other forcing agents; ‘model uncertainty’ associated with climate models; internal variability and initial condition uncertainty; and forcing and boundary condition uncertainty for the assessment of historical and paleoclimate simulations (e.g., {'Collins_and_Allen_2002 Collins and Allen, 2002}; {'Yip_et_al_2011 Yip et al., 2011}).

Model uncertainty is an important contributor to uncertainty in climate predictions and projections. It includes, but is not restricted to, the uncertainties introduced by errors in the model’s representation of dynamical and physical and bio-geochemical aspects of the climate system as well as in the model’s response to external forcing. The phrase ‘model uncertainty’ is a common term in the climate change literature, but different studies use the phrase in different senses: some use it to represent the range of behaviours observed in ensembles of climate model (model spread), while others use it in more comprehensive senses (see Sections 9.2, 11.2 and 12.2). Model spread is often used as a measure of climate response uncertainty, but such a measure is crude as it takes no account of factors such as model quality (Chapter 9) or model independence (e.g., {'Masson_and_Knutti_2011 Masson and Knutti, 2011}; {'Pennell_and_Reichler_2011 Pennell and Reichler, 2011}), and not all variables of interest are adequately simulated by global climate models.

To maintain a degree of terminological clarity this report distinguishes between ‘model spread’ for this narrower representation of climate model responses and ‘model uncertainty’ which describes uncertainty about the extent to which any particular climate model provides an accurate representation of the real climate system. This uncertainty arises from approximations required in the development of models. Such approximations affect the representation of all aspects of the climate including the response to external forcings.

Model uncertainty is sometimes decomposed further into parametric and structural uncertainty, comprising, respectively, uncertainty in the values of model parameters and uncertainty in the underlying model structure (see Section 12.2). Some scientific research areas, such as detection and attribution and observationally-constrained model projections of future climate, incorporate significant elements of both observational and model-based science, and in these instances both sets of relevant uncertainties need to be incorporated.

Scenario uncertainty refers to the uncertainties that arise due to limitations in our understanding of future emissions, concentration or forcing trajectories. Scenarios help in the assessment of future developments in complex systems that are either inherently unpredictable, or that have high scientific uncertainties ({'IPCC_2000 IPCC, 2000}). The societal choices defining future climate drivers are surrounded by considerable uncertainty, and these are explored by examining the climate response to a wide range of possible futures. In past reports, emissions scenarios from the SRES (IPCC, 2000) were used as the main way of exploring uncertainty in future anthropogenic climate drivers. Recent research has made use of Representative Concentration Pathways (RCP) (van Vuuren et al., {'van_Vuuren_et_al_2011a 2011a}, {'van_Vuuren_et_al_2011b 2011b}).

Internal or natural variability, the natural fluctuations in climate, occur in the absence of any RF of the Earth’s climate ({'Hawkins_and_Sutton_2009 Hawkins and Sutton, 2009}). Climate varies naturally on nearly all time and space scales, and quantifying precisely the nature of this variability is challenging, and is characterized by considerable uncertainty. The analysis of internal and forced contributions to recent climate is discussed in Chapter 10. The fractional contribution of internal variability compared with other forms of uncertainty varies in time and in space, but usually diminishes with time as other sources of uncertainty become more significant (Hawkins and Sutton, 2009; see also {Ch11 Chapter 11} and {FAQ11 FAQ 1.1}).

In the WGI contribution to the AR5, uncertainty is quantified using 90% uncertainty intervals unless otherwise stated. The 90% uncertainty interval, reported in square brackets, is expected to have a 90% likelihood of covering the value that is being estimated. The value that is being estimated has a 5% likelihood of exceeding the upper endpoint of the uncertainty interval, and the value has a 5% likelihood of being less than that the lower endpoint of the uncertainty interval. A best estimate of that value is also given where available. Uncertainty intervals are not necessarily symmetric about the corresponding best estimate.

In a subject as complex and diverse as climate change, the information available as well as the way it is expressed, and often the interpretation of that material, varies considerably with the scientific context. In some cases, two studies examining similar material may take different approaches even to the quantification of uncertainty. The interpretation of similar numerical ranges for similar variables can differ from study to study. Readers are advised to pay close attention to the caveats and conditions that surround the results presented in peer-reviewed studies, as well as those presented in this assessment. To help readers in this complex and subtle task, the IPCC draws on specific, calibrated language scales to express uncertainty ({'Mastrandrea_et_al_2010 Mastrandrea et al., 2010}), as well as specific procedures for the expression of uncertainty (see Table 1.2). The aim of these structures is to provide tools through which chapter teams might consistently express uncertainty in key results.

## Treatment of Uncertainty in IPCC

In the course of the IPCC assessment procedure, chapter teams review the published research literature, document the findings (including uncertainties), assess the scientific merit of this information, identify the key findings, and attempt to express an appropriate measure of the uncertainty that accompanies these findings using a shared guidance procedure. This process has changed over time. The early Assessment Reports (FAR and SAR) were largely qualitative. As the  eld has grown and matured, uncertainty is being treated more explicitly, with a greater emphasis on the expression, where possible and appropriate, of quantified measures of uncertainty.

Although IPCC’s treatment of uncertainty has become more sophisticated since the early reports, the rapid growth and considerable diversity of climate research literature presents ongoing challenges. In the wake of the TAR the IPCC formed a Cross-Working Group team charged with identifying the issues and compiling a set of Uncertainty Guidance Notes that could provide a structure for consistent treatment of uncertainty across the IPCC’s remit ({'Manning_et_al_2004 Manning et al., 2004}). These expanded on the procedural elements of {'Moss_and_Schneider_2000 Moss and Schneider (2000)} and introduced calibrated language scales designed to enable chapter teams to use the appropriate level of precision to describe findings. These notes were revised between the TAR and AR4 and again between AR4 and AR5 ({'Mastrandrea_et_al_2010 Mastrandrea et al., 2010}).

Recently, increased engagement of social scientists (e.g., {'Patt_and_Schrag_2003 Patt and Schrag, 2003}; {'Kandlikar_et_al_2005 Kandlikar et al., 2005}; {'Risbey_and_Kandlikar_2007 Risbey and Kandlikar, 2007}; {'Broomell_and_Budescu_2009 Broomell and Budescu, 2009}; {'Budescu_et_al_2009 Budescu et al., 2009}; {'CCSP_2009 CCSP, 2009}) and expert advisory panels (CCSP, 2009; {'InterAcademyCouncil_2010 InterAcademy Council, 2010}) in the area of uncertainty and climate change has helped clarify issues and procedures to improve presentation of uncertainty. Many of the recommendations of these groups are addressed in the revised Guidance Notes. One key revision relates to clarification of the relationship between the ‘confidence’ and ‘likelihood’ language, and pertains to demarcation between qualitative descriptions of ‘confidence’ and the numerical representations of uncertainty that are expressed by the likelihood scale. In addition, a finding that includes a probabilistic measure of uncertainty does not require explicit mention of the level of confidence associated with that finding if the level of confidence is high or very high. This is a concession to stylistic clarity and readability: if something is described as having a high likelihood, then in the absence of additional qualifiers it should be inferred that it also has high or very high confidence.

## Uncertainty Treatment in This Assessment

All three IPCC Working Groups in the AR5 have agreed to use two metrics for communicating the degree of certainty in key findings ({'Mastrandrea_et_al_2010 Mastrandrea et al., 2010}):

  * confidence in the validity of a finding, based on the type, amount, quality, and consistency of evidence (e.g., data, mechanistic understanding, theory, models, expert judgment) and the degree of agreement. Confidence is expressed qualitatively.
  * Quantified measures of uncertainty in a finding expressed probabilistically (based on statistical analysis of observations or model results, or expert judgement).

A level of confidence synthesizes the Chapter teams’ judgements about the validity of findings as determined through evaluation of the available evidence and the degree of scientific agreement. The evidence and agreement scale underpins the assessment, as it is on the basis of evidence and agreement that statements can be made with scientific confidence (in this sense, the evidence and agreement scale replaces the ‘level of scientific understanding’ scale used in previous WGI assessments). There is flexibility in this relationship; for a given evidence and agreement statement, different confidence levels could be assigned, but increasing levels of evidence and degrees of agreement are correlated with increasing confidence. Confidence cannot necessarily be assigned for all combinations of evidence and agreement, but where key variables are highly uncertain, the available evidence and scientific agreement regarding that variable are presented and discussed. Confidence should not be interpreted probabilistically, and it is distinct from ‘statistical confidence’.

The confidence level is based on the evidence (robust, medium and limited) and the agreement (high, medium and low). A combination of different methods, e.g., observations and modelling, is important for evaluating the confidence level. Figure 1.11 shows how the combined evidence and agreement results in five levels for the confidence level used in this assessment.

{image:'Fig11}

The qualifier ‘likelihood’ provides calibrated language for describing quantified uncertainty. It can be used to express a probabilistic estimate of the occurrence of a single event or of an outcome, for example, a climate parameter, observed trend, or projected change lying in a given range. Statements made using the likelihood scale may be based on statistical or modelling analyses, elicitation of expert views, or other quantitative analyses. Where sufficient information is available it is preferable to eschew the likelihood qualifier in favour of the full probability distribution or the appropriate probability range. See Table 1.2 for the list of ‘likelihood’ qualifiers to be used in AR5.

Many social sciences studies have found that the interpretation of uncertainty is contingent on the presentation of information, the context within which statements are placed and the interpreter’s own lexical preferences. Readers often adjust their interpretation of probabilistic language according to the magnitude of perceived potential consequences ({'Patt_and_Schrag_2003 Patt and Schrag, 2003}; {'Patt_and_Dessai_2005 Patt and Dessai, 2005}). Furthermore, the framing of a probabilistic statement impinges on how it is interpreted ({'Kahneman_and_Tversky_1979 Kahneman and Tversky, 1979}): for example, a 10% chance of dying is interpreted more negatively than a 90% chance of surviving.

In addition, work examining expert judgement and decision making shows that people—including scientific experts—are prone to a range of heuristics and biases that affect their judgement (e.g., {'Kahneman_et_al_1982 Kahneman et al., 1982}). For example, in the case of expert judgements there is a tendency towards overconfidence both at the individual level (Morgan et al., 1990) and at the group level as people converge on a view and draw confidence in its reliability from each other. However, in an assessment of the state of scientific knowledge across a  eld such as climate change—characterized by complexity of process and heterogeneity of data constraints—some degree of expert judgement is inevitable ({'Mastrandrea_et_al_2010 Mastrandrea et al., 2010}).

These issues were brought to the attention of chapter teams so that contributors to the AR5 might be sensitized to the ways presentation, framing, context and potential biases might affect their own assessments and might contribute to readers’ understanding of the information presented in this assessment. There will always be room for debate about how to summarize such a large and growing literature. The uncertainty guidance is aimed at providing a consistent, calibrated set of words through which to communicate the uncertainty, confidence and degree of consensus prevailing in the scientific literature. In this sense the guidance notes and practices adopted by IPCC for the presentation of uncertainties should be regarded as an interdisciplinary work in progress, rather than as a finalized, comprehensive approach. Moreover, one precaution that should be considered is that translation of this assessment from English to other languages may lead to a loss of precision.
